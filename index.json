
[{"content":"","date":"2025/12/11","externalUrl":null,"permalink":"/tags/blackwell/","section":"Tags","summary":"","title":"Blackwell","type":"tags"},{"content":"","date":"2025/12/11","externalUrl":null,"permalink":"/tags/build/","section":"Tags","summary":"","title":"Build","type":"tags"},{"content":"","date":"2025/12/11","externalUrl":null,"permalink":"/","section":"Cory's blog","summary":"","title":"Cory's blog","type":"page"},{"content":"","date":"2025/12/11","externalUrl":null,"permalink":"/tags/lammps/","section":"Tags","summary":"","title":"Lammps","type":"tags"},{"content":" 在Blackwell架构显卡上构建带pair_nequip_allegro的lammps # 目标 # 在5090服务器上构建带pair_nequip_allegro的lammps\n范围 # 适用于要求提供带pair_nequip_allegro的lammps\n定义 # Name explain 资源 需要使用lammps的机器或者集群 互联网 直能够连接github 所需 # 已经拥有资源的ssh操作权限，不需要root权限 资源使用X86-64的CPU 操作系统版本大于等于ubuntu20.04/rocky8,主要限制为glibc版本需要2.31及以上 gcc版本大于等于gcc11,以便于支持c++17 操作者电脑能够连接互联网或者资源本身可以连接互联网 资源已安装miniforge 可选：cuda12.8或更高版本以及适配的cudnn已经安装至资源上 使用/bin/bash执行所有命令 能够访问国内镜像源 https://help.mirrors.cernet.edu.cn/pypi/ https://mirrors.aliyun.com/pytorch-wheels/cu128/ https://mirrors.sustech.edu.cn/help/anaconda.html lammps/allegro/pair_nequip_allegro需要使用2025年之后发行的版本 一个具备读写权限的目录，目录大小大于10g 操作步骤 # 1.环境准备 # 环境变量确认 环境 确认步骤 失败时 conda 可以正常使用conda activate或者提示conda命令不存在 删除~.bashrc中关于conda初始化的内容后再次开启终端 cuda nvcc -V 报告版本大于等于12.8 安装cuda12.8或者通过conda配置cuda和cudnn gcc x86_64-linux-gnu-g++ -v和x86_64-linux-gnu-gcc -v报告版本大于11.4 编译安装gcc11.4或者通过conda安装,如果具备root权限则可以在操作系统上直接安装 手动配置环境变量 CONDA_ENV_NAME\n配置lammps的conda环境名称 LAMMPS_VER\nlammps安装版本 PAIR_NEQUIP_ALLEGRO_VER\npair_nequip_allegro安装版本 ALL_PREFIX 操作进行的目录，需要保证资源账号具备读写权限 版本匹配 # lammps必须与pair_nequip_allegro的版本相匹配\n例如:\nLAMMPS_VER=stable_22Jul2025_update2 PAIR_NEQUIP_ALLEGRO_VER=v0.7.0 配置源码目录 LAMMPS_SRC=${ALL_PREFIX}/lammps_${LAMMPS_VER} ALLEGRO_SRC=${ALL_PREFIX}/allegro PAIR_NEQUIP_ALLEGRO_SRC=${ALL_PREFIX}/pair_nequip_allegro_${PAIR_NEQUIP_ALLEGRO_VER} 配置镜像源 ALI_MIRROR=\u0026#34;https://mirrors.aliyun.com/pytorch-wheels/cu128/\u0026#34; export PIP_INDEX_URL=https://mirrors.cernet.edu.cn/pypi/web/simple LAMMPS_GIT=https://github.com/lammps/lammps.git ALLEGRO_GIT=https://github.com/mir-group/allegro.git PAIR_NEQUIP_ALLEGRO_GIT=https://github.com/mir-group/pair_nequip_allegro.git 安装并启用conda环境 确认conda镜像源\nconda config --show channel_priority 使用的源模式,灵活选择模式为flexible，严格选择模式为strict\nconda config --show channels 使用的conda源\n当使用灵活选择模式时，只需要保证镜像源中具备可以访问的镜像即可，否则需要调整源的顺序确保defaults在最后\n调整方式为修改~/.condarc文件 channels: - defaults show_channel_urls: true default_channels: - https://mirrors.sustech.edu.cn/anaconda/pkgs/main - https://mirrors.sustech.edu.cn/anaconda/pkgs/free - https://mirrors.sustech.edu.cn/anaconda/pkgs/r - https://mirrors.sustech.edu.cn/anaconda/pkgs/pro - https://mirrors.sustech.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.sustech.edu.cn/anaconda/cloud msys2: https://mirrors.sustech.edu.cn/anaconda/cloud bioconda: https://mirrors.sustech.edu.cn/anaconda/cloud menpo: https://mirrors.sustech.edu.cn/anaconda/cloud pytorch: https://mirrors.sustech.edu.cn/anaconda/cloud simpleitk: https://mirrors.sustech.edu.cn/anaconda/cloud nvidia: https://mirrors.sustech.edu.cn/anaconda-extra/cloud conda create -n ${CONDA_ENV_NAME} python=3.11 openmpi \u0026#34;libblas=*=*_mkl\u0026#34; \u0026#34;cmake\u0026lt;=3.29.4\u0026#34; -y conda activate ${CONDA_ENV_NAME} 安装pytorch pip install -f ${ALI_MIRROR} \\ \u0026#34;numpy\u0026lt;2\u0026#34; \\ torch==2.7.1+cu128 torchvision torchaudio 确认pytorch安装正确 # python -c \u0026quot;import torch; print(torch._C._GLIBCXX_USE_CXX11_ABI)\u0026quot; 应当输出true\npython -c 'import torch;print(torch.utils.cmake_prefix_path)' 应当输出路径\n源码下载\n配置git代理 git config --global http.proxy http://127.0.0.1:7897 git config --global https.proxy http://127.0.0.1:7897 下载源码\ngit clone -b ${LAMMPS_VER} ${LAMMPS_GIT} ${LAMMPS_SRC} git clone ${ALLEGRO_GIT} ${ALLEGRO_SRC} git clone ${PAIR_NEQUIP_ALLEGRO_GIT} ${PAIR_NEQUIP_ALLEGRO_SRC} 手动配置文件\n将下列两个文件拷贝至工作目录下备用 nvcc_wrapper # cp ${LAMMPS_SRC}/lib/kokkos/bin/nvcc_wrapper .\n变量 值 default_arch sm_120 host_compiler /usr/bin/x86_64-linux-gnu-g++ cuda_args -Xnvcc \u0026ndash;expt-extended-lambda cuda.cmake # 如果torch版本大于等于2.8.0则不需要执行此步骤\ncp ${CONDA_PREFIX}/lib/python${PYTHON_VER}/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake .\n寻找USE_SYSTEM_NVTX,在其上方开启set(USE_SYSTEM_NVTX ON),在内部强制替换nvtx3路径为find_path(nvtx3_dir NAMES nvtx3 PATHS \u0026quot;/data/apps/cuda/12.8/include\u0026quot;)\n2.构建allegro # 先决条件 # 资源具备GPU 资源可以联网 cp cuda.cmake ${CONDA_PREFIX}/lib/python${PYTHON_VER}/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake cd $ALLEGRO_SRC pip install -v \u0026#34;numpy\u0026lt;2\u0026#34; . 3.构建lammps # 先决条件 # 资源具备GPU NVVM配置 确保which cicc能够找到正确的nvvm路径 # 添加cicc到PATH export PATH=$CUDA_HOME/nvvm/bin:$PATH export LD_LIBRARY_PATH=$CUDA_HOME/nvvm/lib64:$LD_LIBRARY_PATH export LIBRARY_PATH=$CUDA_HOME/nvvm/lib64:$LIBRARY_PATH export CPATH=$CUDA_HOME/nvvm/include:$CPATH OpenMPI配置编译器 确保mpicc -v和mpicxx -v能够正常输出编译器版本\nfortran编译器不是必选项 export CC=/usr/bin/x86_64-linux-gnu-gcc export CXX=/usr/bin/x86_64-linux-gnu-g++ export FC=/usr/bin/x86_64-linux-gnu-gfortran export OMPI_CC=$CC export OMPI_CXX=$CXX export OMPI_FC=$FC 清理环境变量\nCMAKE_PREFIX_PATH unset CMAKE_PREFIX_PATH Caffe2/pytorch环境变量 export CUDNN_ROOT=${CUDNN_HOME} export CUDA_ROOT=${CUDA_HOME} export TORCH_CUDA_ARCH_LIST=\u0026#34;10.0;12.0\u0026#34; export USE_CUDNN=1 配置C++编译器\ncp nvcc_wrapper ${LAMMPS_SRC}/lib/kokkos/bin/nvcc_wrapper\n添加allegro插件\ncd $PAIR_NEQUIP_ALLEGRO_SRC chmod +x patch_lammps.sh ./patch_lammps.sh $LAMMPS_SRC cmake配置 key value explain CMAKE_BUILD_TYPE Debug 使用debug模式方便排查错误,如果要求提高速度再换成release版本 CMAKE_INSTALL_PREFIX ${CONDA_PREFIX} lammps的安装目录,没有特殊要求则直接包入conda环境中 CMAKE_CXX_STANDARD 17 c++标准 CMAKE_CXX_COMPILER $LAMMPS_SRC/lib/kokkos/bin/nvcc_wrapper 编译时使用的c++编译器 CMAKE_PREFIX_PATH \u0026quot;$CUDA_HOME;$CONDA_PREFIX/lib/python$PYTHON_VER/site-packages/torch/share/cmake\u0026quot; CMAKE_CUDA_ARCHITECTURES \u0026ldquo;120\u0026rdquo; 120对应5090,100对应B100/B200/B300 GPU_ARCH sm_120 CMAKE_LIBRARY_PATH $CUDA_HOME/lib64/stubs 登录节点编译时使用的选项 CMAKE_MPI_C_COMPILER mpicc CMAKE_MPI_CXX_COMPILER mpicxx FFT KISS 使用lammps内置的傅里叶变换库 BUILD_MPI ON 启用MPI支持 BUILD_OMP ON 启用OpenMP支持 PKG_OPENMP ON OpenMP的CPU多线程加速的pair/style、compute、fix等算子 MKL_INCLUDE_DIR \u0026quot;$CONDA_PREFIX/include\u0026quot; mkl此前已经通过conda安装,因此定义到conda环境中即可 PKG_GPU ON 开启GPU加速 GPU_API cuda GPU调用方式为CUDA CUDAToolkit_ROOT $CUDA_HOME CUDA_TOOLKIT_ROOT_DIR CUDA_HOME PKG_KOKKOS ON 启用kokkos Kokkos_ENABLE_CUDA ON Kokkos_ARCH_AMDAVX ON Kokkos_ARCH_BLACKWELL120 ON Kokkos_ENABLE_CUDA_LAMBDA ON 允许在 Kokkos 的 CUDA 后端里使用 C++ lambda 表达式作为 GPU kernel Kokkos_ENABLE_OPENMP ON NEQUIP_AOT_COMPILE ON 提前编译NequIP/Allegro模型算子 cmake \\ -D CMAKE_BUILD_TYPE=Debug \\ -D CMAKE_CXX_STANDARD=17 \\ -D CMAKE_CUDA_STANDARD=17 \\ -D CMAKE_CXX_COMPILER=$LAMMPS_SRC/lib/kokkos/bin/nvcc_wrapper \\ -D BUILD_MPI=ON \\ -D PKG_KOKKOS=ON \\ -D PKG_GPU=ON \\ -D CUDA_ARCH_LIST=12.0 \\ -D CUDAToolkit_ROOT=$CUDA_HOME \\ -D CUDA_TOOLKIT_ROOT_DIR=$CUDA_HOME \\ -D FFT=KISS \\ -D GPU_API=cuda \\ -D CMAKE_CUDA_ARCHITECTURES=\u0026#34;120\u0026#34; \\ -D GPU_ARCH=sm_120 \\ -D Kokkos_ENABLE_CUDA=ON \\ -D Kokkos_ARCH_AMDAVX=ON \\ -D Kokkos_ARCH_BLACKWELL120=ON \\ -D Kokkos_ENABLE_CUDA_LAMBDA=ON \\ -D MKL_INCLUDE_DIR=\u0026#34;$CONDA_PREFIX/include\u0026#34; \\ -D CMAKE_PREFIX_PATH=\u0026#34;$CUDA_HOME;$CONDA_PREFIX/lib/python$PYTHON_VER/site-packages/torch/share/cmake\u0026#34; \\ -D CMAKE_LIBRARY_PATH=$CUDA_HOME/lib64/stubs \\ -D CMAKE_MPI_C_COMPILER=mpicc \\ -D CMAKE_MPI_CXX_COMPILER=mpicxx \\ -D CMAKE_INSTALL_PREFIX=$CONDA_PREFIX \\ -D Kokkos_ENABLE_OPENMP=yes \\ -D BUILD_OMP=ON \\ -D PKG_OPENMP=ON \\ -D NEQUIP_AOT_COMPILE=ON \\ $LAMMPS_SRC/cmake 编译并安装 make -j16 VERBOSE=1 if [ $? -ne 0 ]; then echo \u0026#34;CMake configuration failed.\u0026#34; exit 1 fi echo \u0026#34;CMake configuration succeeded.\u0026#34; echo \u0026#34;install path: $CONDA_PREFIX\u0026#34; make install 附件 # 5090-HPC安装脚本 # ","date":"2025/12/11","externalUrl":null,"permalink":"/posts/lammps_pair_nequip_allegro_blackwell/","section":"Posts","summary":"","title":"Lammps with pair_nequip_allegro in blackwell","type":"posts"},{"content":"","date":"2025/12/11","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2025/12/11","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2025/12/11","externalUrl":null,"permalink":"/tags/x86/","section":"Tags","summary":"","title":"X86","type":"tags"},{"content":" 基于CUDA 12.9和SM_100的VLLM环境搭建 # 环境构建 # Container # 基础镜像：以nvcr.io/nvidia/pytorch:25.06-py3@06aa7e7a6f5a为基础构建容器\n根据pytorch-25.06 release info 该镜像提供下列工具\nName version CUDA 12.9.1 Torch-TensorRT 2.8.0a0 NVIDIA DALI® 1.50 nvImageCodec 0.2.0.7 MAGMA 2.6.2 JupyterLab 4.3.6 TensorRT Model Optimizer 0.29 TransformerEngine 2.4 NVIDIA RAPIDS™ 25.04 NVIDIA cuBLASMp 0.4.0 Ubuntu 24.04 GCC 13.3.0 建议在此基础上构建镜像以便于提供服务\n例如：\nARG NGC_VERSION=25.06-py3 FROM nvcr.io/nvidia/pytorch:${NGC_VERSION} AS base ENV DEBIAN_FRONTEND=noninteractive RUN mkdir /run/sshd \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y \\ openssh-server \\ openssh-client \\ vim \\ wget \\ git ENV TZ=Asia/Shanghai ENV NCCL_IB_RETRY_CNT=\u0026#34;13\u0026#34; NCCL_IB_TIMEOUT=\u0026#34;22\u0026#34; \\ NCCL_DEBUG=\u0026#34;WARN\u0026#34; \\ NCCL_IB_DISABLE=\u0026#34;0\u0026#34; RUN apt install -y net-tools sudo RUN dpkg-statoverride --remove /usr/lib/dbus-1.0/dbus-daemon-launch-helper || true CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;] 继续工作 # sshd # mkdir /run/sshd \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y openssh-server # Start by /usr/sbin/sshd # Or Start as a service /usr/sbin/sshd -D 如果通过服务启动sshd,则可以通过下列命令管理sshd\n# 查看状态 service ssh status # 重启服务 service ssh restart # 关闭服务 service ssh stop # 启动服务 service ssh start # 共支持方法为{start|stop|reload|force-reload|restart|try-restart|status} Conda # 通过miniforge来安装conda\n将 Miniforge3-24.1.2-0-Linux-x86_64.sh文件上传至容器内执行安装\n也可以在miniforge release上选择其他版本\nchmod +x ./Miniforge3-24.1.2-0-Linux-x86_64.sh ./Miniforge3-24.1.2-0-Linux-x86_64.sh ~/.bashrc # 建议使用南科大源作为镜像源\n因为该镜像源包含了pytorch和nvidia\nchannels: - defaults show_channel_urls: true default_channels: - https://mirrors.sustech.edu.cn/anaconda/pkgs/main - https://mirrors.sustech.edu.cn/anaconda/pkgs/free - https://mirrors.sustech.edu.cn/anaconda/pkgs/r - https://mirrors.sustech.edu.cn/anaconda/pkgs/pro - https://mirrors.sustech.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.sustech.edu.cn/anaconda/cloud msys2: https://mirrors.sustech.edu.cn/anaconda/cloud bioconda: https://mirrors.sustech.edu.cn/anaconda/cloud menpo: https://mirrors.sustech.edu.cn/anaconda/cloud pytorch: https://mirrors.sustech.edu.cn/anaconda/cloud simpleitk: https://mirrors.sustech.edu.cn/anaconda/cloud nvidia: https://mirrors.sustech.edu.cn/anaconda-extra/cloud VLLM安装 # conda环境 # 容器内的python构建在根目录下，建议从conda中重新构建环境 通过bash命令切换进入bash环境\n执行以下命令创建并进入一个新的conda环境\n其中${CONDA_ENV_NAME}为您设定的conda环境名称\nbash conda create -n ${CONDA_ENV_NAME} python=3.12 conda activate ${CONDA_ENV_NAME} pytorch安装 # 使用最新的2.8.0版本torch\npip install \\ \u0026#34;numpy\u0026lt;2\u0026#34; \\ torch==2.8.0 torchvision==0.23.0 \\ --index-url https://download.pytorch.org/whl/cu129 VLLM和transformer安装 # 请通过预构建好的vllm二进制发行包来安装\npip install \\ \u0026#34;numpy\u0026lt;2\u0026#34; \\ \u0026#34;transformers\u0026lt;4.54.0\u0026#34; \\ vllm-0.9.3.dev0+ga5dd03c1e.d20250819.cu129-cp312-cp312-linux_x86_64.whl 测试 # 推理服务 # 通过下列命令启动vllm的推理服务\npython -m vllm.entrypoints.openai.api_server \\ --served-model-name Qwen2-7B-Instruct \\ --model /root/crr/Qwen2.5-7B-Instruct \\ --gpu-memory-utilization 0.2 \\ --port 20000 接口测试 # 下列命令如果没有任何返回则证明服务已正常运行\ncurl http://127.0.0.1:20000/health 下列命令可以与模型进行对话\ncurl http://127.0.0.1:20000/v1/chat/completions \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen2-7B-Instruct\u0026#34;, \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你好\u0026#34; }], \u0026#34;stream\u0026#34;: false }\u0026#39; 编译vllm # 环境准备 # 参考前文章节 vllm安装-conda环境-pytorch安装\nvllm源码 # 获取 # git clone -b v0.9.2 https://github.com/vllm-project/vllm.git 修改 # pyproject.toml # 删除9行附近的 \u0026quot;torch == 2.7.0\u0026quot;,\nsetup.cfg # 添加setup.cfg\n[easy_install] index_url = https://mirrors.bfsu.edu.cn/pypi/web/simple requirements/build.txt # 删除torch==2.7.0\nrequirements/cuda.txt # 删除下列行\ntorch==2.7.0 torchaudio==2.7.0 # These must be updated alongside torch torchvision==0.22.0 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version # https://github.com/facebookresearch/xformers/releases/tag/v0.0.30 xformers==0.0.30; platform_system == \u0026#39;Linux\u0026#39; and platform_machine == \u0026#39;x86_64\u0026#39; # Requires PyTorch \u0026gt;= 2.7 使用现有pytorch安装 # *可选项 安装ccache加速c++编译 # apt install -y ccache 或通过conda安装\nconda install conda-forge::ccache 根据vllm官方文档\n通过下列命令修改源码文件的依赖项配置\ngit clone -b v0.9.2 https://github.com/vllm-project/vllm.git cd vllm python use_existing_torch.py 开始编译，如果内存足够大,则加大MAX_JOBS来加速编译\npip install -r requirements/build.txt mkdir -p ../whl MAX_JOBS=8 python setup.py bdist_wheel -d ../whl 2\u0026gt;\u0026amp;1 | tee ../build_whl.log 如果之前有失败的编译记录，则需要先清除上次的构建文件夹\ncd vllm rm -rf build/* ","date":"2025/08/21","externalUrl":null,"permalink":"/posts/b200-vllm/","section":"Posts","summary":"","title":"B200 Vllm","type":"posts"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm","type":"tags"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/tags/vllm/","section":"Tags","summary":"","title":"Vllm","type":"tags"},{"content":" LAMMPS with mace # 资源 # 4090\n环境创建 # 这个比较麻烦，torch不支持torch2.6，最大只能支持到2.4.0\n因此pytorch里面没有带cxx11-abi，必须使用libtorch\nlibtorch选择https://download.pytorch.org/libtorch/cu124/libtorch-shared-with-deps-2.4.0%2Bcu124.zip\n同时源码文件使用git clone --branch=mace --depth=1 https://github.com/ACEsuit/lammps ${LAMMPS_SRC}\n同时kokkos_arch中没有爱达·拉芙蕾丝,因此通过GPU_ARCH=sm_89 决定架构\n构建环境使用env_create.sh\n编译 # 编译lammps # sbatch build_lammps.sh\n编译mace # sbatch build_mace.sh\n测试 # sbatch run.sh\n","date":"2025/08/20","externalUrl":null,"permalink":"/posts/lammps_with_mace/","section":"Posts","summary":"","title":"Lammps_with_mace","type":"posts"},{"content":"","date":"2025/08/13","externalUrl":null,"permalink":"/tags/ops/","section":"Tags","summary":"","title":"Ops","type":"tags"},{"content":"","date":"2025/08/13","externalUrl":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":" 起因 # 客户在winserver2019机器上执行http.client.HTTPSConnection(\u0026quot;xingchen-api.xf-yun.com\u0026quot;, timeout=120) 命令时报错\n错误为\n[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) 该报错表示SSL上下文无法验证服务器的SSL证书颁发机构,根本原因是找不到本地证书\n一线解决方案 # 一线执行pip install --upgrade certifi试图修复错误，失败\n禁用证书验证测试连通性,正常\nimport ssl http.client.HTTPSConnection( \u0026#34;xingchen-api.xf-yun.com\u0026#34;, timeout=120, context=ssl._create_unverified_context()) 最终解决方案 # 通过pip install pip_system_certs安装解决\n","date":"2025/08/13","externalUrl":null,"permalink":"/posts/windows-ssl-error/","section":"Posts","summary":"","title":"一例Python发起HTTPS请求出现SSL报错","type":"posts"},{"content":"","date":"2025/08/11","externalUrl":null,"permalink":"/tags/container/","section":"Tags","summary":"","title":"Container","type":"tags"},{"content":" Docker带IB和GPU安装 # 参考 # 流程 # 安装docker #!/bin/bash sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${UBUNTU_CODENAME:-$VERSION_CODENAME}\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker version 配置NVIDIA_CONTAINER_TOOLKIT #!/bin/bash distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \u0026amp;\u0026amp; \\ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \u0026amp;\u0026amp; \\ curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update export NVIDIA_CONTAINER_TOOLKIT_VERSION=1.17.8-1 sudo apt-get install -y \\ nvidia-container-toolkit=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ nvidia-container-toolkit-base=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container-tools=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container1=${NVIDIA_CONTAINER_TOOLKIT_VERSION} sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker 主机sshd配置\n修改 /etc/ssh/sshd_config以允许远程主机通过ssh隧道向容器内提供服务 GatewayPorts clientspecified docker网络配置\n配置 /etc/systemd/system/docker.service.d/http-proxy.conf 以允许docker通过代理服务拉取镜像 [Service] Environment=\u0026#34;HTTP_PROXY=http://127.0.0.1:7897\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://127.0.0.1:7897\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.1\u0026#34; 配置 ~/.docker/config.json 以允许容器内使用宿主机的代理服务,其中MASTER_ADDR应设置为 ip a|grep bond0|grep 24|awk '{print $2}'|awk -F/ '{print $1}'获取的值\n{ \u0026#34;proxies\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;httpProxy\u0026#34;: \u0026#34;http://${MASTER_ADDR}:7897\u0026#34;, \u0026#34;httpsProxy\u0026#34;: \u0026#34;http://${MASTER_ADDR}$:7897\u0026#34;, \u0026#34;noProxy\u0026#34;: \u0026#34;localhost,127.0.0.1/8,10.20.200.1/8\u0026#34; } } } sudo mkdir -p /etc/systemd/system/docker.service.d cp http-proxy.conf /etc/systemd/system/docker.service.d mkdir -p /root/.docker cp config.json /root/.docker/ sudo systemctl daemon-reload sudo systemctl restart docker docker info|grep -i proxy MTU配置\n还是修改 ~/.docker/config.json 配置合理的MTU以便Docker通过宿主机联接互联网 { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://docker.m.daocloud.io\u0026#34;], \u0026#34;mtu\u0026#34;:1442, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } overlay网络创建 头结点配置 # export MASTER_ADDR=\u0026#34;10.20.20.11\u0026#34; docker swarm init --advertise-addr $MASTER_ADDR # docker swarm join --token SWMTKN-1-4f4899ad7l558ida8qet6jgswuyqa3nli89udora7warw302qh-6gu0scglazcoy11p9bqumxnor 10.20.20.11:2377 docker network create -d overlay --subnet=\u0026#34;10.20.200.0/24\u0026#34; --gateway=\u0026#34;10.20.200.1\u0026#34; --attachable overlay01 # nktlph09p0f8e9fcdqotw15d8 从节点配置 # docker swarm join --token SWMTKN-1-4f4899ad7l558ida8qet6jgswuyqa3nli89udora7warw302qh-6gu0scglazcoy11p9bqumxnor 10.20.20.11:2377 配置NGC密钥 登录(NGC)[https://org.ngc.nvidia.com/]\n在setup\u0026gt;API Keys中点击 Generate Personal Key获取密钥 docker login nvcr.io Username:$oauthtoken Password:XXX 运行NCCL-tests Dockerfile # ARG NGC_VERSION=23.04-py3 FROM nvcr.io/nvidia/pytorch:${NGC_VERSION} AS base ENV DEBIAN_FRONTEND=noninteractive RUN mkdir /run/sshd \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y \\ openssh-server \\ openssh-client \\ vim \\ wget \\ git RUN git clone https://github.com/NVIDIA/nccl-tests.git /nccl-tests \u0026amp;\u0026amp; cd /nccl-tests \u0026amp;\u0026amp; make MPI=1 MPI_HOME=/opt/hpcx/ompi/ -j ENV TZ=Asia/Shanghai ENV NCCL_IB_RETRY_CNT=\u0026#34;13\u0026#34; NCCL_IB_TIMEOUT=\u0026#34;22\u0026#34; \\ NCCL_DEBUG=\u0026#34;WARN\u0026#34; \\ NCCL_IB_HCA=\u0026#34;mlx5_0:1,mlx5_1:1,mlx5_4:1,mlx5_5:1\u0026#34; \\ NCCL_IB_P2P_DISABLE=\u0026#34;0\u0026#34; \\ NCCL_IB_DISABLE=\u0026#34;0\u0026#34; RUN apt install -y net-tools sudo RUN dpkg-statoverride --remove /usr/lib/dbus-1.0/dbus-daemon-launch-helper || true 制作本地docker文件 # docker pull nvcr.io/nvidia/pytorch:23.04-py3 docker pull nvcr.io/nvidia/pytorch:25.06-py3 docker save -o pytorch.tar nvcr.io/nvidia/pytorch:23.04-py3 docker build -t nccl-test:v2025.08.12 . docker save -o /data/dockerimage/nccl-test_2025.08.12.tar nccl-test:v2025.08.12 1号节点执行 # docker run -itd --runtime=nvidia --gpus all --device=/dev/infiniband --shm-size 1024G --ulimit memlock=-1 --network overlay01 \\ --ip 10.20.200.100 \\ -v /data:/data -v /root:/root -v /root/.ssh/:/root/.ssh/ c81e26d88f9b EX_ID=`docker ps --latest -q` 其余节点执行 # docker load -i /data/dockerimage/pytorch_23.04-py3.tar docker load -i /data/dockerimage/nccl-test_2025.08.12.tar docker run -itd --runtime=nvidia --gpus all --device=/dev/infiniband --shm-size 1024G --ulimit memlock=-1 --network overlay01 \\ --ip 10.20.200.$nodeid \\ -v /data:/data -v /root:/root -v /root/.ssh/:/root/.ssh/ c81e26d88f9b EX_ID=`docker ps --latest -q` docker exec -it ${EX_ID} bash /usr/sbin/sshd 节点互通测试 # ssh -T 10.20.200.101 启动nccl-tests # EX_ID=`docker ps --latest -q` docker exec -it ${EX_ID} bash cd cd /data/apps/nccl-tests-2.16.7_ubuntu2002/ mpirun --allow-run-as-root \\ -np 32 -H 10.20.200.100:8,10.20.200.101:8,10.20.200.102:8,10.20.200.103:8 \\ ./build/all_reduce_perf -g 1 -b 512M -e 16G -f 2 2\u0026gt;\u0026amp;1 |tee test4node.log ","date":"2025/08/11","externalUrl":null,"permalink":"/posts/docker_gpu_ib/","section":"Posts","summary":"","title":"带IB和GPU的docker安装","type":"posts"},{"content":"","date":"2025/08/10","externalUrl":null,"permalink":"/tags/debug/","section":"Tags","summary":"","title":"Debug","type":"tags"},{"content":"","date":"2025/08/10","externalUrl":null,"permalink":"/tags/torch/","section":"Tags","summary":"","title":"Torch","type":"tags"},{"content":" 现象 # 提交作业，输出3个循环后不再打印日志\nnvidia-smi和普罗米修斯报告作业的gpu利用率为100%\n集群使用的GPU为Tesla-V100\n排查 # print()发现卡死位置为x_prime = torch.linalg.solve(A_reg, b_prime)\n结合现象,考虑是矩阵求解时卡死 对输入变量进行检查\nprint(\u0026#34;check A_reg: {}\u0026#34;.format(torch.isnan(A_reg).any())) print(\u0026#34;check b_prime: {}\u0026#34;.format(torch.isnan(b_prime).any())) ","date":"2025/08/10","externalUrl":null,"permalink":"/posts/pytorch-deadlock/","section":"Posts","summary":"","title":"一例pytorch死锁排查","type":"posts"},{"content":" numpy必须使用1.x cmake 对于nvtx的支持存在问题,必须手动指定 find_path(nvtx3_dir NAMES nvtx3 PATHS \u0026#34;/data/apps/cuda/12.6/include\u0026#34;) 使用conda安装mpi时,默认使用了mpich,mpich不提供OMPI_系列的环境变量,因此需要手动指定openmpi作为mpi后端 项目地址:pair_nequip_allegro 建议使用nvcc_warpper作为编译前端,路径在${LAMMPS_SRC}/lib/kokkos/bin/nvcc_wrapper default_arch=\u0026#34;sm_89\u0026#34; host_compiler=\u0026#39;${CONDA_PREFIX}/bin/x86_64-conda-linux-gnu-g++\u0026#39; ","date":"2025/08/09","externalUrl":null,"permalink":"/posts/lammps_build_nequip/","section":"Posts","summary":"","title":"Lammps build with nequip","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/topics/","section":"Topics","summary":"","title":"Topics","type":"topics"}]