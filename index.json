
[{"content":" 基于CUDA 12.9和SM_100的VLLM环境搭建 # 环境构建 # Container # 基础镜像：以nvcr.io/nvidia/pytorch:25.06-py3@06aa7e7a6f5a为基础构建容器\n根据pytorch-25.06 release info 该镜像提供下列工具\nName version CUDA 12.9.1 Torch-TensorRT 2.8.0a0 NVIDIA DALI® 1.50 nvImageCodec 0.2.0.7 MAGMA 2.6.2 JupyterLab 4.3.6 TensorRT Model Optimizer 0.29 TransformerEngine 2.4 NVIDIA RAPIDS™ 25.04 NVIDIA cuBLASMp 0.4.0 Ubuntu 24.04 GCC 13.3.0 建议在此基础上构建镜像以便于提供服务\n例如：\nARG NGC_VERSION=25.06-py3 FROM nvcr.io/nvidia/pytorch:${NGC_VERSION} AS base ENV DEBIAN_FRONTEND=noninteractive RUN mkdir /run/sshd \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y \\ openssh-server \\ openssh-client \\ vim \\ wget \\ git ENV TZ=Asia/Shanghai ENV NCCL_IB_RETRY_CNT=\u0026#34;13\u0026#34; NCCL_IB_TIMEOUT=\u0026#34;22\u0026#34; \\ NCCL_DEBUG=\u0026#34;WARN\u0026#34; \\ NCCL_IB_DISABLE=\u0026#34;0\u0026#34; RUN apt install -y net-tools sudo RUN dpkg-statoverride --remove /usr/lib/dbus-1.0/dbus-daemon-launch-helper || true CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;] 继续工作 # sshd # mkdir /run/sshd \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y openssh-server # Start by /usr/sbin/sshd # Or Start as a service /usr/sbin/sshd -D 如果通过服务启动sshd,则可以通过下列命令管理sshd\n# 查看状态 service ssh status # 重启服务 service ssh restart # 关闭服务 service ssh stop # 启动服务 service ssh start # 共支持方法为{start|stop|reload|force-reload|restart|try-restart|status} Conda # 通过miniforge来安装conda\n将 Miniforge3-24.1.2-0-Linux-x86_64.sh文件上传至容器内执行安装\n也可以在miniforge release上选择其他版本\nchmod +x ./Miniforge3-24.1.2-0-Linux-x86_64.sh ./Miniforge3-24.1.2-0-Linux-x86_64.sh ~/.bashrc # 建议使用南科大源作为镜像源\n因为该镜像源包含了pytorch和nvidia\nchannels: - defaults show_channel_urls: true default_channels: - https://mirrors.sustech.edu.cn/anaconda/pkgs/main - https://mirrors.sustech.edu.cn/anaconda/pkgs/free - https://mirrors.sustech.edu.cn/anaconda/pkgs/r - https://mirrors.sustech.edu.cn/anaconda/pkgs/pro - https://mirrors.sustech.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.sustech.edu.cn/anaconda/cloud msys2: https://mirrors.sustech.edu.cn/anaconda/cloud bioconda: https://mirrors.sustech.edu.cn/anaconda/cloud menpo: https://mirrors.sustech.edu.cn/anaconda/cloud pytorch: https://mirrors.sustech.edu.cn/anaconda/cloud simpleitk: https://mirrors.sustech.edu.cn/anaconda/cloud nvidia: https://mirrors.sustech.edu.cn/anaconda-extra/cloud VLLM安装 # conda环境 # 容器内的python构建在根目录下，建议从conda中重新构建环境 通过bash命令切换进入bash环境\n执行以下命令创建并进入一个新的conda环境\n其中${CONDA_ENV_NAME}为您设定的conda环境名称\nbash conda create -n ${CONDA_ENV_NAME} python=3.12 conda activate ${CONDA_ENV_NAME} pytorch安装 # 使用最新的2.8.0版本torch\npip install \\ \u0026#34;numpy\u0026lt;2\u0026#34; \\ torch==2.8.0 torchvision==0.23.0 \\ --index-url https://download.pytorch.org/whl/cu129 VLLM和transformer安装 # 请通过预构建好的vllm二进制发行包来安装\npip install \\ \u0026#34;numpy\u0026lt;2\u0026#34; \\ \u0026#34;transformers\u0026lt;4.54.0\u0026#34; \\ vllm-0.9.3.dev0+ga5dd03c1e.d20250819.cu129-cp312-cp312-linux_x86_64.whl 测试 # 推理服务 # 通过下列命令启动vllm的推理服务\npython -m vllm.entrypoints.openai.api_server \\ --served-model-name Qwen2-7B-Instruct \\ --model /root/crr/Qwen2.5-7B-Instruct \\ --gpu-memory-utilization 0.2 \\ --port 20000 接口测试 # 下列命令如果没有任何返回则证明服务已正常运行\ncurl http://127.0.0.1:20000/health 下列命令可以与模型进行对话\ncurl http://127.0.0.1:20000/v1/chat/completions \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen2-7B-Instruct\u0026#34;, \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你好\u0026#34; }], \u0026#34;stream\u0026#34;: false }\u0026#39; 编译vllm # 环境准备 # 参考前文章节 vllm安装-conda环境-pytorch安装\nvllm源码 # 获取 # git clone -b v0.9.2 https://github.com/vllm-project/vllm.git 修改 # pyproject.toml # 删除9行附近的 \u0026quot;torch == 2.7.0\u0026quot;,\nsetup.cfg # 添加setup.cfg\n[easy_install] index_url = https://mirrors.bfsu.edu.cn/pypi/web/simple requirements/build.txt # 删除torch==2.7.0\nrequirements/cuda.txt # 删除下列行\ntorch==2.7.0 torchaudio==2.7.0 # These must be updated alongside torch torchvision==0.22.0 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version # https://github.com/facebookresearch/xformers/releases/tag/v0.0.30 xformers==0.0.30; platform_system == \u0026#39;Linux\u0026#39; and platform_machine == \u0026#39;x86_64\u0026#39; # Requires PyTorch \u0026gt;= 2.7 使用现有pytorch安装 # *可选项 安装ccache加速c++编译 # apt install -y ccache 或通过conda安装\nconda install conda-forge::ccache 根据vllm官方文档\n通过下列命令修改源码文件的依赖项配置\ngit clone -b v0.9.2 https://github.com/vllm-project/vllm.git cd vllm python use_existing_torch.py 开始编译，如果内存足够大,则加大MAX_JOBS来加速编译\npip install -r requirements/build.txt mkdir -p ../whl MAX_JOBS=8 python setup.py bdist_wheel -d ../whl 2\u0026gt;\u0026amp;1 | tee ../build_whl.log 如果之前有失败的编译记录，则需要先清除上次的构建文件夹\ncd vllm rm -rf build/* ","date":"2025/08/21","externalUrl":null,"permalink":"/posts/b200-vllm/","section":"Posts","summary":"","title":"B200 Vllm","type":"posts"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/tags/build/","section":"Tags","summary":"","title":"Build","type":"tags"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/","section":"Cory's blog","summary":"","title":"Cory's blog","type":"page"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm","type":"tags"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2025/08/21","externalUrl":null,"permalink":"/tags/vllm/","section":"Tags","summary":"","title":"Vllm","type":"tags"},{"content":"","date":"2025/08/20","externalUrl":null,"permalink":"/tags/lammps/","section":"Tags","summary":"","title":"Lammps","type":"tags"},{"content":" LAMMPS with mace # 资源 # 4090\n环境创建 # 这个比较麻烦，torch不支持torch2.6，最大只能支持到2.4.0\n因此pytorch里面没有带cxx11-abi，必须使用libtorch\nlibtorch选择https://download.pytorch.org/libtorch/cu124/libtorch-shared-with-deps-2.4.0%2Bcu124.zip\n同时源码文件使用git clone --branch=mace --depth=1 https://github.com/ACEsuit/lammps ${LAMMPS_SRC}\n同时kokkos_arch中没有爱达·拉芙蕾丝,因此通过GPU_ARCH=sm_89 决定架构\n构建环境使用env_create.sh\n编译 # 编译lammps # sbatch build_lammps.sh\n编译mace # sbatch build_mace.sh\n测试 # sbatch run.sh\n","date":"2025/08/20","externalUrl":null,"permalink":"/posts/lammps_with_mace/","section":"Posts","summary":"","title":"Lammps_with_mace","type":"posts"},{"content":"","date":"2025/08/13","externalUrl":null,"permalink":"/tags/ops/","section":"Tags","summary":"","title":"Ops","type":"tags"},{"content":"","date":"2025/08/13","externalUrl":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":" 起因 # 客户在winserver2019机器上执行http.client.HTTPSConnection(\u0026quot;xingchen-api.xf-yun.com\u0026quot;, timeout=120) 命令时报错\n错误为\n[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) 该报错表示SSL上下文无法验证服务器的SSL证书颁发机构,根本原因是找不到本地证书\n一线解决方案 # 一线执行pip install --upgrade certifi试图修复错误，失败\n禁用证书验证测试连通性,正常\nimport ssl http.client.HTTPSConnection( \u0026#34;xingchen-api.xf-yun.com\u0026#34;, timeout=120, context=ssl._create_unverified_context()) 最终解决方案 # 通过pip install pip_system_certs安装解决\n","date":"2025/08/13","externalUrl":null,"permalink":"/posts/windows-ssl-error/","section":"Posts","summary":"","title":"一例Python发起HTTPS请求出现SSL报错","type":"posts"},{"content":"","date":"2025/08/11","externalUrl":null,"permalink":"/tags/container/","section":"Tags","summary":"","title":"Container","type":"tags"},{"content":" Docker带IB和GPU安装 # 参考 # 流程 # 安装docker #!/bin/bash sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${UBUNTU_CODENAME:-$VERSION_CODENAME}\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker version 配置NVIDIA_CONTAINER_TOOLKIT #!/bin/bash distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \u0026amp;\u0026amp; \\ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \u0026amp;\u0026amp; \\ curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update export NVIDIA_CONTAINER_TOOLKIT_VERSION=1.17.8-1 sudo apt-get install -y \\ nvidia-container-toolkit=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ nvidia-container-toolkit-base=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container-tools=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container1=${NVIDIA_CONTAINER_TOOLKIT_VERSION} sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker 主机sshd配置\n修改 /etc/ssh/sshd_config以允许远程主机通过ssh隧道向容器内提供服务 GatewayPorts clientspecified docker网络配置\n配置 /etc/systemd/system/docker.service.d/http-proxy.conf 以允许docker通过代理服务拉取镜像 [Service] Environment=\u0026#34;HTTP_PROXY=http://127.0.0.1:7897\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://127.0.0.1:7897\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.1\u0026#34; 配置 ~/.docker/config.json 以允许容器内使用宿主机的代理服务,其中MASTER_ADDR应设置为 ip a|grep bond0|grep 24|awk '{print $2}'|awk -F/ '{print $1}'获取的值\n{ \u0026#34;proxies\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;httpProxy\u0026#34;: \u0026#34;http://${MASTER_ADDR}:7897\u0026#34;, \u0026#34;httpsProxy\u0026#34;: \u0026#34;http://${MASTER_ADDR}$:7897\u0026#34;, \u0026#34;noProxy\u0026#34;: \u0026#34;localhost,127.0.0.1/8,10.20.200.1/8\u0026#34; } } } sudo mkdir -p /etc/systemd/system/docker.service.d cp http-proxy.conf /etc/systemd/system/docker.service.d mkdir -p /root/.docker cp config.json /root/.docker/ sudo systemctl daemon-reload sudo systemctl restart docker docker info|grep -i proxy MTU配置\n还是修改 ~/.docker/config.json 配置合理的MTU以便Docker通过宿主机联接互联网 { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://docker.m.daocloud.io\u0026#34;], \u0026#34;mtu\u0026#34;:1442, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } overlay网络创建 头结点配置 # export MASTER_ADDR=\u0026#34;10.20.20.11\u0026#34; docker swarm init --advertise-addr $MASTER_ADDR # docker swarm join --token SWMTKN-1-4f4899ad7l558ida8qet6jgswuyqa3nli89udora7warw302qh-6gu0scglazcoy11p9bqumxnor 10.20.20.11:2377 docker network create -d overlay --subnet=\u0026#34;10.20.200.0/24\u0026#34; --gateway=\u0026#34;10.20.200.1\u0026#34; --attachable overlay01 # nktlph09p0f8e9fcdqotw15d8 从节点配置 # docker swarm join --token SWMTKN-1-4f4899ad7l558ida8qet6jgswuyqa3nli89udora7warw302qh-6gu0scglazcoy11p9bqumxnor 10.20.20.11:2377 配置NGC密钥 登录(NGC)[https://org.ngc.nvidia.com/]\n在setup\u0026gt;API Keys中点击 Generate Personal Key获取密钥 docker login nvcr.io Username:$oauthtoken Password:XXX 运行NCCL-tests Dockerfile # ARG NGC_VERSION=23.04-py3 FROM nvcr.io/nvidia/pytorch:${NGC_VERSION} AS base ENV DEBIAN_FRONTEND=noninteractive RUN mkdir /run/sshd \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y \\ openssh-server \\ openssh-client \\ vim \\ wget \\ git RUN git clone https://github.com/NVIDIA/nccl-tests.git /nccl-tests \u0026amp;\u0026amp; cd /nccl-tests \u0026amp;\u0026amp; make MPI=1 MPI_HOME=/opt/hpcx/ompi/ -j ENV TZ=Asia/Shanghai ENV NCCL_IB_RETRY_CNT=\u0026#34;13\u0026#34; NCCL_IB_TIMEOUT=\u0026#34;22\u0026#34; \\ NCCL_DEBUG=\u0026#34;WARN\u0026#34; \\ NCCL_IB_HCA=\u0026#34;mlx5_0:1,mlx5_1:1,mlx5_4:1,mlx5_5:1\u0026#34; \\ NCCL_IB_P2P_DISABLE=\u0026#34;0\u0026#34; \\ NCCL_IB_DISABLE=\u0026#34;0\u0026#34; RUN apt install -y net-tools sudo RUN dpkg-statoverride --remove /usr/lib/dbus-1.0/dbus-daemon-launch-helper || true 制作本地docker文件 # docker pull nvcr.io/nvidia/pytorch:23.04-py3 docker pull nvcr.io/nvidia/pytorch:25.06-py3 docker save -o pytorch.tar nvcr.io/nvidia/pytorch:23.04-py3 docker build -t nccl-test:v2025.08.12 . docker save -o /data/dockerimage/nccl-test_2025.08.12.tar nccl-test:v2025.08.12 1号节点执行 # docker run -itd --runtime=nvidia --gpus all --device=/dev/infiniband --shm-size 1024G --ulimit memlock=-1 --network overlay01 \\ --ip 10.20.200.100 \\ -v /data:/data -v /root:/root -v /root/.ssh/:/root/.ssh/ c81e26d88f9b EX_ID=`docker ps --latest -q` 其余节点执行 # docker load -i /data/dockerimage/pytorch_23.04-py3.tar docker load -i /data/dockerimage/nccl-test_2025.08.12.tar docker run -itd --runtime=nvidia --gpus all --device=/dev/infiniband --shm-size 1024G --ulimit memlock=-1 --network overlay01 \\ --ip 10.20.200.$nodeid \\ -v /data:/data -v /root:/root -v /root/.ssh/:/root/.ssh/ c81e26d88f9b EX_ID=`docker ps --latest -q` docker exec -it ${EX_ID} bash /usr/sbin/sshd 节点互通测试 # ssh -T 10.20.200.101 启动nccl-tests # EX_ID=`docker ps --latest -q` docker exec -it ${EX_ID} bash cd cd /data/apps/nccl-tests-2.16.7_ubuntu2002/ mpirun --allow-run-as-root \\ -np 32 -H 10.20.200.100:8,10.20.200.101:8,10.20.200.102:8,10.20.200.103:8 \\ ./build/all_reduce_perf -g 1 -b 512M -e 16G -f 2 2\u0026gt;\u0026amp;1 |tee test4node.log ","date":"2025/08/11","externalUrl":null,"permalink":"/posts/docker_gpu_ib/","section":"Posts","summary":"","title":"带IB和GPU的docker安装","type":"posts"},{"content":"","date":"2025/08/10","externalUrl":null,"permalink":"/tags/debug/","section":"Tags","summary":"","title":"Debug","type":"tags"},{"content":"","date":"2025/08/10","externalUrl":null,"permalink":"/tags/torch/","section":"Tags","summary":"","title":"Torch","type":"tags"},{"content":" 现象 # 提交作业，输出3个循环后不再打印日志\nnvidia-smi和普罗米修斯报告作业的gpu利用率为100%\n集群使用的GPU为Tesla-V100\n排查 # print()发现卡死位置为x_prime = torch.linalg.solve(A_reg, b_prime)\n结合现象,考虑是矩阵求解时卡死 对输入变量进行检查\nprint(\u0026#34;check A_reg: {}\u0026#34;.format(torch.isnan(A_reg).any())) print(\u0026#34;check b_prime: {}\u0026#34;.format(torch.isnan(b_prime).any())) ","date":"2025/08/10","externalUrl":null,"permalink":"/posts/pytorch-deadlock/","section":"Posts","summary":"","title":"一例pytorch死锁排查","type":"posts"},{"content":" numpy必须使用1.x cmake 对于nvtx的支持存在问题,必须手动指定 find_path(nvtx3_dir NAMES nvtx3 PATHS \u0026#34;/data/apps/cuda/12.6/include\u0026#34;) 使用conda安装mpi时,默认使用了mpich,mpich不提供OMPI_系列的环境变量,因此需要手动指定openmpi作为mpi后端 项目地址:pair_nequip_allegro 建议使用nvcc_warpper作为编译前端,路径在${LAMMPS_SRC}/lib/kokkos/bin/nvcc_wrapper default_arch=\u0026#34;sm_89\u0026#34; host_compiler=\u0026#39;${CONDA_PREFIX}/bin/x86_64-conda-linux-gnu-g++\u0026#39; ","date":"2025/08/09","externalUrl":null,"permalink":"/posts/lammps_build_nequip/","section":"Posts","summary":"","title":"Lammps build with nequip","type":"posts"},{"content":"","date":"2025/08/09","externalUrl":null,"permalink":"/tags/x86/","section":"Tags","summary":"","title":"X86","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/topics/","section":"Topics","summary":"","title":"Topics","type":"topics"}]